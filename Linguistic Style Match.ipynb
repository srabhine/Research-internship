{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df9c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwords = dict(\n",
    "    auxiliary_verbs=['am',\n",
    "                    'had',\n",
    "                    'could',\n",
    "                     'is',\n",
    "                    'did',\n",
    "                    'does',\n",
    "                    'are',\n",
    "                    'shall',\n",
    "                    'do',\n",
    "                    'was',\n",
    "                    'will',\n",
    "                    'need',\n",
    "                    'were',\n",
    "                    'should',\n",
    "                    'ought to',\n",
    "                    'being',\n",
    "                    'would',\n",
    "                    'dare',\n",
    "                    'been',\n",
    "                    'may',\n",
    "                    'going to',\n",
    "                    'be',\n",
    "                    'might',\n",
    "                    \"be able to\",\n",
    "                    \"has\",\n",
    "                    \"must\",\n",
    "                    \"have to\",\n",
    "                    \"have\",\n",
    "                    \"can\",\n",
    "                    \"had better\"],\n",
    "    articles=['a',\n",
    "              'an',\n",
    "              'the'],\n",
    "    \n",
    "    common_adverbs=\n",
    "        ['accidentally',\n",
    "         'afterwards',\n",
    "         'almost',\n",
    "         'always',\n",
    "         'angrily',\n",
    "         'annually',\n",
    "         'anxiously',\n",
    "         'awkwardly',\n",
    "         'badly',\n",
    "         'blindly',\n",
    "         'boastfully',\n",
    "         'boldly',\n",
    "         'bravely',\n",
    "         'briefly',\n",
    "         'brightly',\n",
    "         'crossly',\n",
    "         'cruelly',\n",
    "         'daily',\n",
    "         'defiantly',\n",
    "         'deliberately',\n",
    "         'doubtfully',\n",
    "         'easily',\n",
    "         'elegantly',\n",
    "         'enormously',\n",
    "         'enthusiastically',\n",
    "         'equally',\n",
    "         'even',\n",
    "         'eventually',\n",
    "         'exactly',\n",
    "         'faithfully',\n",
    "         'gladly',\n",
    "         'gracefully',\n",
    "         'greedily',\n",
    "         'happily',\n",
    "         'hastily',\n",
    "         'honestly',\n",
    "         'hourly',\n",
    "         'hungrily',\n",
    "         'innocently',\n",
    "         'inquisitively',\n",
    "         'irritably',\n",
    "         'joyously',\n",
    "         'justly',\n",
    "         'kindly',\n",
    "         'lazily',\n",
    "         'nearly',\n",
    "         'neatly',\n",
    "         'nervously',\n",
    "         'never',\n",
    "         'noisily',\n",
    "         'not',\n",
    "         'obediently',\n",
    "         'obnoxiously',\n",
    "         'often',\n",
    "         'only',\n",
    "         'painfully',\n",
    "         'perfectly',\n",
    "         'politely',\n",
    "         'poorly',\n",
    "         'powerfully',\n",
    "         'reluctantly',\n",
    "         'repeatedly',\n",
    "         'rightfully',\n",
    "         'roughly',\n",
    "         'rudely',\n",
    "         'sadly',\n",
    "         'safely',\n",
    "         'seldom',\n",
    "        'selfishly',\n",
    "        'seriously',\n",
    "        'shakily',\n",
    "        'sharply',\n",
    "        'shrilly',\n",
    "        'shyly',\n",
    "        'silently',\n",
    "        'sternly',\n",
    "        'successfully',\n",
    "        'suddenly',\n",
    "        'suspiciously',\n",
    "        'swiftly',\n",
    "        'tenderly',\n",
    "        'tensely',\n",
    "        'thoughtfully',\n",
    "        'tightly',\n",
    "        'tomorrow',\n",
    "        'too',\n",
    "        'truthfully',\n",
    "        'unexpectedly',\n",
    "        'very',\n",
    "        'victoriously'],\n",
    "    \n",
    "    personal_pronouns=[\"i\",\n",
    "                       \"you\",\n",
    "                       \"he\",\n",
    "                       \"she\",\n",
    "                       \"it\",\n",
    "                       \"we\",\n",
    "                       \"they\",\n",
    "                       \"them\",\n",
    "                       \"us\", \n",
    "                       \"him\", \n",
    "                       \"her\", \n",
    "                       \"his\", \n",
    "                       \"hers\", \n",
    "                       \"its\", \n",
    "                       \"theirs\", \n",
    "                       \"our\", \n",
    "                       \"your\"],\n",
    "    \n",
    "    impersonal_pronouns=[\"one\",\n",
    "                        \"they\",\n",
    "                        \"it\",\n",
    "                        \"you\"],\n",
    "    \n",
    "    prepositions=['about',\n",
    "                  'above',\n",
    "                  'across',\n",
    "                  'after',\n",
    "                  'ago',\n",
    "                  'at',\n",
    "                  'below',\n",
    "                  'by',\n",
    "                  'down',\n",
    "                  'during',\n",
    "                  'for',\n",
    "                  'from',\n",
    "                  'in',\n",
    "                  'into',\n",
    "                  'off',\n",
    "                  'on',\n",
    "                  'over',\n",
    "                  'past',\n",
    "                  'since',\n",
    "                  'through',\n",
    "                  'to',\n",
    "                  'under',\n",
    "                  'until',\n",
    "                  'up',\n",
    "                  'with'],\n",
    "    \n",
    "    negations=['doesn’t',\n",
    "               'isn’t',\n",
    "               'wasn’t',\n",
    "               'shouldn’t',\n",
    "               'wouldn’t',\n",
    "               'couldn’t',\n",
    "               'won’t',\n",
    "               'can’t',\n",
    "               'don’t',\n",
    "               'hardly',\n",
    "               'scarcely',\n",
    "               'barely',\n",
    "               'no',\n",
    "               'not',\n",
    "               'none',\n",
    "               'no one',\n",
    "               'nobody',\n",
    "               'nothing',\n",
    "               'neither',\n",
    "               'nowhere',\n",
    "               'never'],\n",
    "    conjunctions=['after',\n",
    "                  'after all',\n",
    "                  'although',\n",
    "                  'and',\n",
    "                  'as', \n",
    "                  'as if',\n",
    "                  \"as long as\",\n",
    "                  \"as much as\",\n",
    "                  \"as soon as\",\n",
    "                  \"as though\",\n",
    "                  \"barely when\",\n",
    "                  \"because\",\"Before\",\n",
    "                  \"but\",\n",
    "                  \"by the time\",\n",
    "                  \"bonsequently\",\n",
    "                  \"bum\",\n",
    "                  \"either or\",\n",
    "                  \"even\",\n",
    "                  \"even if\",\n",
    "                  \"even though\",\n",
    "                  \"ever since\",\n",
    "                  \"every time\",\n",
    "                  \"finally\",\n",
    "                  \"for\",\n",
    "                  \"furthermore\",\n",
    "                  \"hardly when\",\n",
    "                  \"hence\",\n",
    "                  \"how\",\n",
    "                  \"however\",\n",
    "                  \"if\",\n",
    "                  'if … then',\n",
    "                  'if only',\n",
    "                  'if then',\n",
    "                  'if when',\n",
    "                  'in addition',\n",
    "                  'in order that',\n",
    "                  'inasmuch',\n",
    "                  'incidentally',\n",
    "                  'just as',\n",
    "                  'lest',\n",
    "                  'likewise',\n",
    "                  'meanwhile',\n",
    "                  'nor',\n",
    "                  'now',\n",
    "                  'now since',\n",
    "                  'now that',\n",
    "                  'now when',\n",
    "                  'once',\n",
    "                  'only if',\n",
    "                  'or',\n",
    "                  'or else',\n",
    "                  'otherwise',\n",
    "                  'provided',\n",
    "                  'provided that',\n",
    "                  'rather than',\n",
    "                  'scarcely when',\n",
    "                  'since'],\n",
    "    \n",
    "    quantifiers=['much',\n",
    "                 'a bit',\n",
    "                 'little',\n",
    "                 'great deal of',\n",
    "                 'large quantity of',\n",
    "                 'a little',\n",
    "                 'very little',\n",
    "                 'a large amount of',\n",
    "                 'a majority of',\n",
    "                 'a great number of',\n",
    "                 'several',\n",
    "                 'many',\n",
    "                 'a large number of',\n",
    "                 'a number of',\n",
    "                 'few',\n",
    "                 'a few',\n",
    "                 'very few',\n",
    "                 'all',\n",
    "                 'enough',\n",
    "                 'none',\n",
    "                 'no',\n",
    "                 'some',\n",
    "                 'more',\n",
    "                 'most',\n",
    "                 'lots of',\n",
    "                 'less',\n",
    "                 'least',\n",
    "                 'any',\n",
    "                 'not any',\n",
    "                 'plenty of']\n",
    ")\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85c79a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import liwc\n",
    "parse, category_names = liwc.load_token_parser('D:/Users/932151706/Desktop/EnDict.dicx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7a3a68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', 'is', 'a', 'good', 'day,', 'we', 'went', 'to', 'school', 'and', 'ate', 'so', 'sugar']\n"
     ]
    }
   ],
   "source": [
    "phrase= \"Today is a good day, we went to school and ate so sugar\"\n",
    "split = phrase.split()\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ddc63ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27899686520376177\n",
      "[0.04493208 0.04702194 0.01044932 0.0522466  0.03030303 0.05642633\n",
      " 0.00522466 0.02194357 0.01044932]\n"
     ]
    }
   ],
   "source": [
    "#FWC function \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df=pd.read_excel('D:/Users/932151706/Desktop/T1_LSM.xlsx')\n",
    "\n",
    "#case 0 nombres de mots dans le dic \n",
    "#case 1 nombres totals de mots dans les tweets \n",
    "\n",
    "tableautot=[0,0] \n",
    "\n",
    "tableaucategories=np.zeros(9)\n",
    "\n",
    "for i in range(len(df['tweet'])):\n",
    "    \n",
    "    split = df['tweet'][i].split()\n",
    "\n",
    "\n",
    "    tableau=[0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "    for i in range(len(split)):\n",
    "        cptcategories=0\n",
    "        for j in (fwords.items()):\n",
    "\n",
    "            if split[i].lower() in j[1]:\n",
    "\n",
    "                #print(split[i])\n",
    "                tableau[cptcategories]+=1\n",
    "\n",
    "            cptcategories+=1\n",
    "    tableautot[0]+=sum(tableau)\n",
    "    tableautot[1]+=len(split)\n",
    "    \n",
    "    tableaucategories+=np.array(tableau)\n",
    "    \n",
    "#print(tableautot)\n",
    "\n",
    "FWC= tableautot[0]/tableautot[1]\n",
    "somme= tableaucategories/tableautot[1]\n",
    "#print(tableaucategories)\n",
    "print(FWC)\n",
    "print(somme)\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15d3f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df=pd.read_csv(\"D:/Users/932151706/.spyder-py3/Tweets14dMcdo.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8711045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          @absolutchrys this is the best! You just post ...\n",
       "1          @mjl69 thank you! GOD is great! I landed a maj...\n",
       "2          Good grief!  Whitney was 1 mile from the torna...\n",
       "3          @askfrasco medical tech visit \\n\\nWww.myspace....\n",
       "4                        @creasly561 you under house arrest?\n",
       "                                 ...                        \n",
       "6108633                    Wake me up when I finish the week\n",
       "6108634                 COMO QUE PODE EXISTIR PESSOAS ASSIM?\n",
       "6108635                     Aiaiai Como Assim? Ahuahushauhsu\n",
       "6108636    RT @washingtonpost: At $12,000 per head, this ...\n",
       "6108637                                                tweet\n",
       "Name: tweet, Length: 6108638, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in df['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "006f21ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2530492542260765\n",
      "[0.03693819 0.03227976 0.00762887 0.054968   0.02071869 0.05282344\n",
      " 0.00705217 0.02974722 0.01089292]\n"
     ]
    }
   ],
   "source": [
    "#FWC function \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df2=pd.read_csv(\"D:/Users/932151706/.spyder-py3/Tweets14dMcdo.csv\",low_memory=False)\n",
    "\n",
    "#case 0 nombres de mots dans le dic \n",
    "#case 1 nombres totals de mots dans les tweets \n",
    "\n",
    "tableautot=[0,0] \n",
    "\n",
    "tableaucategories=np.zeros(9)\n",
    "\n",
    "for i in range(len(df2['tweet'])):\n",
    "    \n",
    "    if type(df2['tweet'][i])== str:\n",
    "        \n",
    "    \n",
    "        split = df2['tweet'][i].split()\n",
    "\n",
    "\n",
    "        tableau=[0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "        for i in range(len(split)):\n",
    "            cptcategories=0\n",
    "            for j in (fwords.items()):\n",
    "\n",
    "                if split[i].lower() in j[1]:\n",
    "\n",
    "                    #print(split[i])\n",
    "                    tableau[cptcategories]+=1\n",
    "\n",
    "                cptcategories+=1\n",
    "        tableautot[0]+=sum(tableau)\n",
    "        tableautot[1]+=len(split)\n",
    "\n",
    "        tableaucategories+=np.array(tableau)\n",
    "\n",
    "    #print(tableautot)\n",
    "\n",
    "FWC2= tableautot[0]/tableautot[1]\n",
    "somme= tableaucategories/tableautot[1]\n",
    "#print(tableaucategories)\n",
    "print(FWC2)\n",
    "print(somme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd59c5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.753148029704797\n"
     ]
    }
   ],
   "source": [
    "#LSM\n",
    "LSM= 1-(abs(FWC-FWC2/abs(FWC+FWC2+0.0001)))\n",
    "print(LSM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
